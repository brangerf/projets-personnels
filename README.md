# Projets d'Exploration sur les Agents IA Locaux

Ce dépôt regroupe un ensemble de projets dédiés à l'interaction avec des modèles de langage fonctionnant localement via Ollama. Le projet principal, **`full_app_prototype`**, a servi de matrice à partir de laquelle plusieurs modules spécialisés ont vu le jour. Chacun de ces modules a été développé de manière indépendante afin d'isoler, de tester et d'affiner une technologie particulière avant son intégration potentielle dans l'application centrale. Le fichier texte rédigé en grammaire GBNF est un extrait de mes recherches personnelles sur les system_prompts, avant l'apparition des premiers modèles de raisonnements.

### A propos de `full_app_prototype`

Ce logiciel de bureau est conçu pour interagir avec des intelligences artificielles locales. Sa spécificité réside dans sa capacité à orchestrer des équipes d'agents IA pour résoudre des problèmes en plusieurs étapes. Lorsqu'une demande s'avère trop complexe pour une réponse unique, un module coordinateur intelligent, baptisé **Maestro**, prend le relais. Tel un chef de projet, il analyse la requête de l'utilisateur, la segmente en une série de missions distinctes et génère à la volée un schéma de travail complet. Ce plan connecte différents agents spécialisés, chacun doté d'un rôle précis, et définit la manière dont les informations circulent de l'un à l'autre jusqu'à la résolution finale. Le système peut même corriger ce plan de manière autonome s'il y décèle des incohérences. Pour assurer une restitution claire, un dernier agent intervient pour synthétiser et mettre en forme l'ensemble des résultats, livrant ainsi un rapport unifié et lisible. L'interface graphique offre la possibilité de visualiser et de modifier ces schémas sur un canevas interactif, de sauvegarder ses propres configurations et de choisir le modèle de langage à utiliser.

### A propos de `chat_and_rag`

Ce projet est le premier module de développement issu du prototype principal. Il se présente sous la forme d'une application de bureau permettant de dialoguer avec des modèles de langage locaux. Sa fonction centrale est de pouvoir poser des questions sur un document personnel. Pour la recherche sémantique, l'approche est plus élaborée que la simple recherche de mots-clés. Le modèle de langage est d'abord utilisé pour extraire les concepts fondamentaux de la question et leurs variations. Ensuite, le programme identifie dans le document les zones où ces termes sont les plus pertinents et concentrés. Ces passages sont évalués selon la diversité des concepts qu'ils contiennent et la proximité des termes. Pour éviter de présenter des informations redondantes, un filtre de similarité Jaccard ne retient que les extraits les plus singuliers, qui formeront le contexte final de la réponse. L'interface de ce module a été volontairement épurée pour se concentrer sur cette mécanique.

### A propos de `only_maestro_prototype`

Ce composant est une autre brique technologique dérivée de l'application principale. Il a été conçu séparément pour perfectionner la génération de réponses structurées et complexes. L'objectif est de développer une approche méthodique de la création de contenu. Au lieu de formuler une réponse monolithique, ce module emploie un "Maestro" qui agit comme un architecte de l'information. Le processus se déroule en deux temps : l'IA est d'abord chargée de concevoir un plan de réponse détaillé, à la manière d'un sommaire. Par la suite, ce plan est automatiquement transformé en un flux de travail multi-agents, où chaque agent se voit assigner la rédaction d'une section spécifique. Pour garantir la fiabilité du processus, le système intègre des mécanismes de robustesse, comme des tentatives multiples et une logique d'analyse pour valider le plan généré. Une attention particulière a été portée à la gestion des contenus techniques, avec une prise en charge du rendu des formules mathématiques grâce à LaTeX.

### A propos de `transcription_prototype`

Ce troisième module spécialisé, également destiné à être intégré à l'application principale, fournit une chaîne de traitement complète allant de la transcription de fichiers audio/vidéo à la génération de documents professionnels. Il s'appuie sur le modèle Whisper d'OpenAI, piloté par un backend Python, avec une interface web gérée par pywebview. Pour gérer les fichiers volumineux, le module segmente automatiquement les longs enregistrements en morceaux plus petits à l'aide de FFmpeg. Il est optimisé pour tirer parti des cartes graphiques NVIDIA (CUDA) mais reste fonctionnel sur CPU. Une fois la transcription brute obtenue, un modèle de langage local intervient pour post-traiter le texte, que ce soit pour l'embellir, le résumer, ou en extraire les thèmes principaux. Sa force réside dans sa capacité à générer des documents structurés au format LaTeX, en corrigeant les erreurs courantes du code généré par l'IA avant de le compiler en un fichier PDF finalisé.

### A propos de `NebuAI_WebUI`

Ce projet plus ancien, qui n'est pas un dérivé du prototype principal, est le fruit d'un travail collaboratif et représente ma première expérience dans la création d'interfaces. Il s'agit d'une application de discussion locale conçue pour dialoguer avec des modèles de langage via Ollama ou l'API d'OpenAI, reposant sur Python et la bibliothèque Dash. Son intérêt principal est de proposer un cadre pour guider et approfondir le raisonnement de l'intelligence artificielle avant qu'elle ne fournisse sa réponse. Pour ce faire, l'utilisateur peut activer des "fonctions de réflexion", comme la "Chaîne de Pensée" ou le "Questionnement Socratique", incitant le modèle à analyser la question sous différents angles. Pour ne pas encombrer la conversation, les blocs de code ou les tableaux sont extraits et présentés via des boutons cliquables, qui ouvrent un visualiseur dédié. L'interface permet également d'interroger le contenu d'un document PDF et de visualiser en détail les étapes du raisonnement du modèle.
